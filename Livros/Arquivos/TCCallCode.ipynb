{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "livroscsv = pd.read_csv(\"C:\\\\Users\\\\w_vic\\\\OneDrive\\\\Documentos\\\\Projects\\\\LeLivros\\\\livro.csv\")\n",
    "descricao = pd.DataFrame(livroscsv.Descrição)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\w_vic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import unidecode\n",
    "\n",
    "stopwords.words('portuguese')\n",
    "stopwords1 = set(stopwords.words('portuguese'))\n",
    "stopwords2 = [ 'a', 'à', 'adeus', 'agora', 'aí', 'ainda', 'além', 'algo', 'alguém', 'algum', 'alguma', 'algumas', 'alguns', 'ali', 'ampla', 'amplas', 'amplo', 'amplos', 'ano', 'anos', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aqui', 'aquilo', 'área', 'as', 'às', 'assim', 'até', 'atrás', 'através', 'baixo', 'bastante', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cá', 'cada', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cima', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'custa', 'da', 'dá', 'dão', 'daquela', 'daquelas', 'daquele', 'daqueles', 'dar', 'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'demais', 'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'devem', 'devendo', 'dever', 'deverá', 'deverão', 'deveria', 'deveriam', 'devia', 'deviam', 'dez', 'dezanove', 'dezasseis', 'dezassete', 'dezoito', 'dia', 'diante', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'dizer', 'do', 'dois', 'dos', 'doze', 'duas', 'dúvida', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'enquanto', 'entre', 'era', 'eram', 'éramos', 'és', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estás', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estiveste', 'estivestes', 'estou', 'etc', 'eu', 'exemplo', 'faço', 'falta', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazendo', 'fazer', 'fazes', 'feita', 'feitas', 'feito', 'feitos', 'fez', 'fim', 'final', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'forma', 'formos', 'fosse', 'fossem', 'fôssemos', 'foste', 'fostes', 'fui', 'geral', 'grande', 'grandes', 'grupo', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'havia', 'hei', 'hoje', 'hora', 'horas', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'la', 'lá', 'lado', 'lhe', 'lhes', 'lo', 'local', 'logo', 'longe', 'lugar', 'maior', 'maioria', 'mais', 'mal', 'mas', 'máximo', 'me', 'meio', 'menor', 'menos', 'mês', 'meses', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muita', 'muitas', 'muito', 'muitos', 'na', 'nada', 'não', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'nem', 'nenhum', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes', 'ninguém', 'nível', 'no', 'noite', 'nome', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'número', 'nunca', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'paucas', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'perto', 'pode', 'pude', 'pôde', 'podem', 'podendo', 'poder', 'poderia', 'poderiam', 'podia', 'podiam', 'põe', 'põem', 'pois', 'ponto', 'pontos', 'por', 'porém', 'porque', 'porquê', 'posição', 'possível', 'possivelmente', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios', 'próxima', 'próximas', 'próximo', 'próximos', 'pude', 'puderam', 'quais', 'quáis', 'qual', 'quando', 'quanto', 'quantos', 'quarta', 'quarto', 'quatro', 'que', 'quê', 'quem', 'quer', 'quereis', 'querem', 'queremas', 'queres', 'quero', 'questão', 'quinta', 'quinto', 'quinze', 'relação', 'sabe', 'sabem', 'são', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sejamos', 'sem', 'sempre', 'sendo', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'sete', 'sétima', 'sétimo', 'seu', 'seus', 'sexta', 'sexto', 'si', 'sido', 'sim', 'sistema', 'só', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'também', 'tampouco', 'tanta', 'tantas', 'tanto', 'tão', 'tarde', 'te', 'tem', 'tém', 'têm', 'temos', 'tendes', 'tendo', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terá', 'terão', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'ti', 'tido', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tiveste', 'tivestes', 'toda', 'todas', 'todavia', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas', 'tudo', 'última', 'últimas', 'último', 'últimos', 'um', 'uma', 'umas', 'uns', 'vai', 'vais', 'vão', 'vários', 'vem', 'vêm', 'vendo', 'vens', 'ver', 'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'vir', 'você', 'vocês', 'vos', 'vós', 'vossa', 'vossas', 'vosso', 'vossos', 'zero', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '_' ]\n",
    "stopwords3 = ['história', 'historia']\n",
    "stopwords = (list(stopwords1) +  stopwords2 + stopwords3)\n",
    "\n",
    "sopwordsnotrepeated = []\n",
    "\n",
    "for i in stopwords:\n",
    "    myword = i\n",
    "    myword = unidecode.unidecode(myword)\n",
    "    if myword not in sopwordsnotrepeated: sopwordsnotrepeated.append(myword)\n",
    "\n",
    "stopwords = sopwordsnotrepeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"stopwords.txt\", stopwords, delimiter=\",\", comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "descricao.Descrição = descricao.Descrição.apply(lambda x: ' '.join([unidecode.unidecode(item)\n",
    "   for item in x.lower().split()\n",
    "   if item not in stopwords and item.isalpha()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicioptbr = pd.read_csv(\"C:\\\\Users\\\\w_vic\\\\OneDrive\\\\Documentos\\\\Projects\\\\LeLivros\\\\dicionarioptbr.txt\")\n",
    "dicioptbr = dicioptbr.values.tolist()\n",
    "anotherdic = []\n",
    "for item in dicioptbr:\n",
    "    myitem = unidecode.unidecode(item[0])\n",
    "    if myitem not in anotherdic:\n",
    "        anotherdic.append(unidecode.unidecode(item[0]))\n",
    "\n",
    "dicioptbr = anotherdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"dicionario.txt\", dicioptbr, delimiter=\",\", comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "#nlp = spacy.load(\"C:\\Program Files\\Python39\\Lib\\site-packages\\pt_core_news_sm\\pt_core_news_sm-2.2.0\")\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "for i in range(len(descricao.Descrição)):\n",
    "  doc = nlp(descricao.Descrição[i])\n",
    "  for token in doc:\n",
    "      if token.lemma_ in dicioptbr: \n",
    "        descricao.Descrição[i] = descricao.Descrição[i].replace(str(token), token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "descricao.Descrição = descricao.Descrição.apply(lambda x: ' '.join([unidecode.unidecode(item)\n",
    "   for item in x.split()\n",
    "   if item not in stopwords\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\w_vic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "word2count = {}\n",
    "for data in descricao.Descrição:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys(): word2count[word] = 1\n",
    "        else: word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "a_file = open(\"word2count2.json\", \"w\", encoding='utf8')\n",
    "a_file = json.dump(word2count, a_file, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "\n",
    "freq_words = heapq.nlargest(41498,word2count, key=word2count.get)\n",
    "\n",
    "x = [] # lista de vetores das palavras\n",
    "\n",
    "for data in descricao.Descrição:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        tf = data.count(word) / len(data.split(\" \"))\n",
    "        idf = math.log(len(descricao) / word2count[word])\n",
    "        tfidf = tf * idf\n",
    "        tfidf = (\"{:.3f}\".format(tfidf)).replace(\"0.000\", \"0\")\n",
    "        vector.append(tfidf)\n",
    "    x.append(vector)\n",
    "    \n",
    "y = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(len(descricao))\n",
    "np.savetxt(\"foo2.csv\", y, delimiter=\",\", comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste para selecionar conforme o texto selecionado pelo usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#frasecheck = \"A História do Universo para quem tem pressa: Do Big Bang às mais recentes descobertas da astronomia! (Série Para quem Tem Pressa) Os grandes mistérios e maravilhas do céu noturno sempre nos fascinaram, intrigaram e divertiram, desde os primeiros passos na Terra. Hoje, continuamos nos esforçando para entender o nosso lugar no cosmos. O século 20 foi palco de importantes e assombrosas descobertas sobre o nosso próprio planeta, o sistema solar, as estrelas e as galáxias. Contudo, ainda buscamos respostas para inúmeras questões – O que é matéria escura? Estamos sozinhos no universo? É possível viajar no tempo? –, e essa busca nos proporciona uma valiosa compreensão da vastidão e das infinitas possibilidades do espaço universal que ainda estamos por descobrir. O universo, considerando-se a sua imensidão, pode ser assustador, mas neste livro de fácil compreensão embarcamos numa viagem incrível através de todas as descobertas astronômicas fundamentais, desde as resultantes de crenças de civilizações antigas até as oriundas de pioneiras e recentes observações das ondas gravitacionais, previstas por Einstein mais de 100 anos atrás. Nunca houve ocasião melhor para começar a entender os mistérios do universo, e este guia essencial do cosmos é o melhor ponto de partida!\".lower()\n",
    "frasecheck = \"A historia de amor entre dois jovens que moravam em cidades diferentes em busca de uma vida mais plena e feliz\".lower()\n",
    "\n",
    "frasecheck = ' '.join([unidecode.unidecode(word) for word in frasecheck.split() if word.lower() not in stopwords and word.isalpha()])\n",
    "\n",
    "docFrase = nlp(frasecheck)\n",
    "for token in docFrase:\n",
    "      if token.lemma_ in dicioptbr:\n",
    "        frasecheck = frasecheck.replace(str(token), token.lemma_)\n",
    "\n",
    "frasecheck = ' '.join([unidecode.unidecode(word) for word in frasecheck.split() if word.lower() not in stopwords and word.isalpha()])\n",
    "\n",
    "#fraseword2count = {}\n",
    "#for word in nltk.word_tokenize(frasecheck):\n",
    "#    if word not in fraseword2count.keys():\n",
    "#        fraseword2count[word] = 1\n",
    "#    else:\n",
    "#        fraseword2count[word] += 1\n",
    "\n",
    "z = []\n",
    "\n",
    "vector = []\n",
    "for word in freq_words:\n",
    "        tf = frasecheck.count(word) / len(frasecheck.split(\" \"))\n",
    "        #idf = math.log(len(frasecheck) / fraseword2count[word])\n",
    "        idf = math.log(len(descricao) / word2count[word])\n",
    "        tfidf = tf * idf\n",
    "        tfidf = (\"{:.3f}\".format(tfidf)).replace(\"0.000\", \"0\")\n",
    "        vector.append(tfidf)\n",
    "z.append(vector)\n",
    "\n",
    "a = np.asarray(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova e Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565\n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"a2.csv\", a, delimiter=\",\", comments='', fmt='%s')\n",
    "print(len(frasecheck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "final = {} # dic\n",
    "\n",
    "count = 1\n",
    "\n",
    "for i in y:\n",
    "    i = i.reshape(1, -1) # representa a minha coleção de frases\n",
    "    a = a.reshape(1, -1) # representa a minha frase qual estou consultando\n",
    "    xx = cosine_similarity(i, a)\n",
    "\n",
    "    final[count] = xx[0][0]\n",
    "    count += 1\n",
    "\n",
    "dados = pd.DataFrame(list(final.items()), columns=['Id', 'Similarity'])\n",
    "#dados.drop('Id', inplace=True, axis=1)\n",
    "relevant = dados.sort_values(by=['Similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"relevant.csv\", dados, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0e49f063d6d232e57a0f5bce5356e7e35441361fe9f3f9772e0035109764d05"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
